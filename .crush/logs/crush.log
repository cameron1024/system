{"time":"2025-08-02T16:06:31.106916645+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":113},"msg":"Getting live provider data"}
{"time":"2025-08-02T16:06:31.466583893+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:06:31.467190589+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.Load","file":"github.com/charmbracelet/crush/internal/config/load.go","line":82},"msg":"No providers configured"}
{"time":"2025-08-02T16:06:31.968793262+01:00","level":"INFO","msg":"OK   20250424200609_initial.sql (612.17µs)"}
{"time":"2025-08-02T16:06:31.969060154+01:00","level":"INFO","msg":"OK   20250515105448_add_summary_message_id.sql (240.41µs)"}
{"time":"2025-08-02T16:06:31.969206218+01:00","level":"INFO","msg":"OK   20250624000000_add_created_at_indexes.sql (132.18µs)"}
{"time":"2025-08-02T16:06:31.969396261+01:00","level":"INFO","msg":"OK   20250627000000_add_provider_to_messages.sql (183.73µs)"}
{"time":"2025-08-02T16:06:31.969399387+01:00","level":"INFO","msg":"goose: successfully migrated database to version: 20250627000000"}
{"time":"2025-08-02T16:06:31.969439269+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:06:31.969447534+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/app.New","file":"github.com/charmbracelet/crush/internal/app/app.go","line":97},"msg":"No agent configuration found"}
{"time":"2025-08-02T16:52:27.888484312+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:52:27.889380754+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:52:27.889388279+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:52:27.889428248+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:52:28.251385988+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:52:28.374169358+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:52:28.374245107+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:52:28.378077564+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:52:28.378195998+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:52:28.380577126+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:52:28.380653567+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:52:39.616236212+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:52:39.616322432+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:52:39.618470507+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:52:39.61854267+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:52:47.534524536+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).processGeneration.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":372},"msg":"failed to generate title","error":"received empty streaming response from OpenAI API - check endpoint configuration"}
{"time":"2025-08-02T16:52:47.535840956+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Run.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":346},"msg":"failed to process events: received empty streaming response from OpenAI API - check endpoint configuration"}
{"time":"2025-08-02T16:53:20.699859681+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:53:20.700729908+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:53:20.700735358+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:53:20.70081701+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:53:20.855046117+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:53:21.218748325+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:53:21.21881664+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:53:21.221630041+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:53:21.221733696+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:53:21.224298757+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:53:21.224373895+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:53:28.11254881+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Run.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":346},"msg":"failed to process events: POST \"http://mini:11112/v1/chat/completions\": 404 Not Found {\n        \"message\": \"Failed to load model \\\"qwen/qwen3-coder-30b\\\". Error: Model does not exist.\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"model\",\n        \"code\": \"model_not_found\"\n    }"}
{"time":"2025-08-02T16:54:02.278322332+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:54:02.279166505+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:54:02.279175893+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:54:02.27925549+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:54:02.441102792+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:54:02.770809161+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:54:02.770877916+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:54:02.774373033+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:54:02.774481126+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:54:02.776674238+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:54:02.776746771+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:54:53.324531053+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12049 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:54:53.324592624+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:54:55.819872789+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12049 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:54:55.819918829+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:54:57.44923263+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"027460e4-4cd7-4064-8621-2fb124efd054"}
{"time":"2025-08-02T16:55:59.686736355+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:55:59.687597536+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:55:59.687608999+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:55:59.687692683+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:55:59.869923774+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:56:00.199012779+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:56:00.199078168+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:56:00.201949566+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:56:00.202037219+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:56:00.204815725+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:56:00.204905491+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:56:03.399734237+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:56:03.399805717+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:56:05.890612404+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:56:05.890660999+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:56:10.773703789+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T16:56:10.773759739+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T16:56:20.46824634+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":4,"max_retries":8}
{"time":"2025-08-02T16:56:20.468301308+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":4,"max_retries":8}
{"time":"2025-08-02T16:56:39.059872927+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"27f13c99-f845-4e06-b081-c45fd51074ea"}
{"time":"2025-08-02T16:56:45.687931641+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).convertMessages","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":157},"msg":"There is a message without content, investigate, this should not happen"}
{"time":"2025-08-02T16:56:49.729981737+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:56:49.730016805+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:56:52.207080578+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:56:52.207122571+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:56:57.094947394+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T16:56:57.094992854+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T16:57:06.787943163+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":4,"max_retries":8}
{"time":"2025-08-02T16:57:06.78799308+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":4,"max_retries":8}
{"time":"2025-08-02T16:57:26.075592017+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":5,"max_retries":8}
{"time":"2025-08-02T16:57:26.075650652+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":5,"max_retries":8}
{"time":"2025-08-02T16:58:04.55896166+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12105 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":6,"max_retries":8}
{"time":"2025-08-02T16:58:04.559036206+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":6,"max_retries":8}
{"time":"2025-08-02T16:58:40.701121779+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"27f13c99-f845-4e06-b081-c45fd51074ea"}
{"time":"2025-08-02T16:58:50.091051857+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:58:50.091931168+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:58:50.091938993+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:58:50.091981666+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:58:50.308737753+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:58:50.587206792+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:58:50.587274645+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:58:50.591259642+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:58:50.591404425+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:58:50.593823658+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:58:50.593888886+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:58:52.252620598+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12073 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:58:52.2526677+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:58:54.730644496+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12073 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:58:54.730698211+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T16:58:56.385089353+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"e18a66c6-a403-44ef-9d80-1b34abd8fdef"}
{"time":"2025-08-02T16:59:54.274122992+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:59:54.275051096+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T16:59:54.27505802+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:59:54.275141804+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T16:59:54.431205356+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T16:59:54.761146867+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T16:59:54.761215421+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T16:59:54.764181858+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T16:59:54.764306963+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T16:59:54.766108253+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T16:59:54.766169793+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T16:59:58.374572744+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T16:59:58.374614816+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:00:00.86394657+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:00:00.863995245+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:00:05.759436064+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:00:05.759473177+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:00:15.438750843+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":4,"max_retries":8}
{"time":"2025-08-02T17:00:15.438821822+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":4,"max_retries":8}
{"time":"2025-08-02T17:00:34.717456788+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":5,"max_retries":8}
{"time":"2025-08-02T17:00:34.717495234+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":5,"max_retries":8}
{"time":"2025-08-02T17:01:13.196576391+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":6,"max_retries":8}
{"time":"2025-08-02T17:01:13.196615919+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":6,"max_retries":8}
{"time":"2025-08-02T17:02:30.072787544+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12040 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":7,"max_retries":8}
{"time":"2025-08-02T17:02:30.072829757+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":7,"max_retries":8}
{"time":"2025-08-02T17:03:28.997068299+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"9d8c4737-e841-4491-9468-2e8c1f1b7141"}
{"time":"2025-08-02T17:03:44.618005218+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:03:44.618859856+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T17:03:44.61886702+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:03:44.618965996+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:03:44.779606502+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:03:45.105222711+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T17:03:45.105295004+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T17:03:45.108344679+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T17:03:45.108510427+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T17:03:45.110442934+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T17:03:45.110518353+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T17:03:48.100931193+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12075 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:03:48.101005751+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:03:50.577139059+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12075 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:03:50.577213266+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:03:53.221668734+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"cefee7c8-80ce-425b-90b4-3bf41f02808a"}
{"time":"2025-08-02T17:04:18.947263431+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:04:18.948071636+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T17:04:18.948077327+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:04:18.948163227+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:04:19.161260142+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:04:19.61638193+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T17:04:19.616460195+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T17:04:19.619099974+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T17:04:19.619161966+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T17:04:19.621745424+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T17:04:19.62183455+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T17:04:23.205691048+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12102 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:04:23.205731688+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:04:25.694303611+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12102 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:04:25.694352177+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:04:30.580283226+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12102 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:04:30.580344267+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:04:31.305284179+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"119fd78d-bb0b-4a47-9ea5-75d68c44fbc8"}
{"time":"2025-08-02T17:05:26.347180082+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:05:26.348103811+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T17:05:26.348114472+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:05:26.348198267+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:05:26.544162382+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:05:26.84808722+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T17:05:26.848165184+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T17:05:26.850777859+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T17:05:26.850886202+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T17:05:26.852743049+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T17:05:26.852818888+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T17:05:30.29354834+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12102 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:05:30.293583749+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:05:32.769406698+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12102 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:05:32.769462148+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:05:35.238004563+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"e3c613ed-d4b5-4114-8c27-5a89761f703d"}
{"time":"2025-08-02T17:06:29.606357256+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":99},"msg":"Using cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:06:29.607192879+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":237},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:06:29.607211907+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/config.(*Config).configureProviders-range1","file":"github.com/charmbracelet/crush/internal/config/load.go","line":257},"msg":"Provider is missing API key, this might be OK for local providers","provider":"local"}
{"time":"2025-08-02T17:06:29.607244481+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.loadProviders.func1","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":103},"msg":"Updating provider cache in background"}
{"time":"2025-08-02T17:06:29.773470874+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/config.saveProvidersInCache","file":"github.com/charmbracelet/crush/internal/config/provider.go","line":48},"msg":"Saving cached provider data","path":"/home/cameron/.local/share/crush/providers.json"}
{"time":"2025-08-02T17:06:30.100923115+01:00","level":"INFO","msg":"goose: no migrations to run. current version: 20250627000000"}
{"time":"2025-08-02T17:06:30.100989946+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/app.(*App).initLSPClients","file":"github.com/charmbracelet/crush/internal/app/lsp.go","line":18},"msg":"LSP clients initialization started in background"}
{"time":"2025-08-02T17:06:30.104949801+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"task"}
{"time":"2025-08-02T17:06:30.105048265+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"task"}
{"time":"2025-08-02T17:06:30.107480396+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":172},"msg":"Initializing agent tools","agent":"coder"}
{"time":"2025-08-02T17:06:30.107557778+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.NewAgent.func1.1","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":174},"msg":"Initialized agent tools","agent":"coder"}
{"time":"2025-08-02T17:06:32.006924894+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:06:32.006962058+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:06:34.496742917+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:06:34.496801202+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:06:39.373554311+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:06:39.373599861+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:06:40.084109867+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"0bd253a2-f6cc-40c1-994d-df7c4d17bbdf"}
{"time":"2025-08-02T17:06:43.247220026+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).convertMessages","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":157},"msg":"There is a message without content, investigate, this should not happen"}
{"time":"2025-08-02T17:06:43.322052632+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:06:43.322090917+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":1,"max_retries":8}
{"time":"2025-08-02T17:06:45.799712042+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:06:45.799749275+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":2,"max_retries":8}
{"time":"2025-08-02T17:06:54.412261564+01:00","level":"ERROR","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).shouldRetry","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":517},"msg":"OpenAI API error","error":"received error while streaming: {\"message\":\"Trying to keep the first 12104 tokens when context the overflows. However, the model is loaded with context length of only 4096 tokens, which is not enough. Try to load the model with a larger context length, or provide a shorter input\"}","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:06:54.412314078+01:00","level":"WARN","source":{"function":"github.com/charmbracelet/crush/internal/llm/provider.(*openaiClient).stream.func1","file":"github.com/charmbracelet/crush/internal/llm/provider/openai.go","line":461},"msg":"Retrying due to rate limit","attempt":3,"max_retries":8}
{"time":"2025-08-02T17:06:54.978203861+01:00","level":"INFO","source":{"function":"github.com/charmbracelet/crush/internal/llm/agent.(*agent).Cancel","file":"github.com/charmbracelet/crush/internal/llm/agent/agent.go","line":240},"msg":"Request cancellation initiated","session_id":"0bd253a2-f6cc-40c1-994d-df7c4d17bbdf"}
